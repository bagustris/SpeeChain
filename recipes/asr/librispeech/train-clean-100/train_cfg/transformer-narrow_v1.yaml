### Author: Heli Qi
### Affiliation: NAIST
### Date: 2022.09
# This configuration is the 1st version of Transformer-narrow:
# 1. Each Transformer block has 4 attention heads and 256 attention dimension
# 2. The ASR model is trained without a CTC branch
# 3. An additional LayerNorm layer is added into PositionalEncoding of TransformerEncoder and TransformerDecoder
# 4. 1K BPE tokens are used as the vocabulary and label smoothness is set to 0.2 for calculating cross-entropy loss


### --- Reference Values --- ###
token_root: datasets/speech_text/librispeech/data/subword/train-clean-100/bpe1k/normal
d_model: 256
num_heads: 4
fdfwd_dim: 2048
dropout: 0.1
### ------------------------ ###


model:
  model_type: asr.ASR
  model_conf:
    customize_conf:
      token_type: subword
      token_vocab: !ref <token_root>/vocab

  module_conf:
    frontend:
      type: mel_fbank
      conf:
        preemphasis: 0.97
        hop_length: 0.010
        win_length: 0.025
        n_mels: 80

    normalize:
      norm_type: global

    specaug:
      freq_mask_width: 15
      freq_mask_num: 4
      time_mask_width: 20
      time_mask_num: 4

    enc_prenet:
      type: conv2d
      conf:
        conv_dims:
          - 64
          - 64
        conv_kernel: 3
        conv_stride: 2
        conv_batchnorm: true
        lnr_dims: !ref <d_model>

    encoder:
      type: transformer
      conf:
        posenc_dropout: !ref <dropout>
        posenc_scale: false
        emb_layernorm: true
        emb_scale: false
        d_model: !ref <d_model>
        num_heads: !ref <num_heads>
        num_layers: 12
        att_dropout: !ref <dropout>
        fdfwd_dim: !ref <fdfwd_dim>
        fdfwd_dropout: !ref <dropout>
        res_dropout: !ref <dropout>
        layernorm_first: true

    dec_emb:
      type: embed
      conf:
        embedding_dim: !ref <d_model>

    decoder:
      type: transformer
      conf:
        posenc_dropout: !ref <dropout>
        posenc_scale: false
        emb_layernorm: true
        emb_scale: false
        d_model: !ref <d_model>
        num_heads: !ref <num_heads>
        num_layers: 6
        att_dropout: !ref <dropout>
        fdfwd_dim: !ref <fdfwd_dim>
        fdfwd_dropout: !ref <dropout>
        res_dropout: !ref <dropout>
        layernorm_first: true

  criterion_conf:
    label_smoothing: 0.2


optim_sches:
  type: noam.Noamlr
  conf:
    optim_type: Adam
    optim_conf:
      betas:
        - 0.9
        - 0.98
      eps: 1.0e-9
    warmup_steps: 16000