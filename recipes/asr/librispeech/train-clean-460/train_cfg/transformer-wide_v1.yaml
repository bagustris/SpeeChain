### Author: Heli Qi
### Affiliation: NAIST
### Date: 2022.09
# This configuration is the 1st version of Transformer-narrow:
# 1. Each Transformer block has 8 attention heads and 512 attention dimension
# 2. The ASR model is trained without a CTC branch
# 3. An additional LayerNorm layer is added into PositionalEncoding of TransformerEncoder and TransformerDecoder
# 4. 5K BPE tokens are used as the vocabulary and label smoothness is set to 0.2 for calculating cross-entropy loss


### --- Reference Values --- ###
# data-related
dataset: librispeech
train_set: train-clean-460
data_root: !ref datasets/<dataset>/data

# waveform-related
wav_format: wav
sample_rate: 16000

# tokenizer-related
txt_format: librispeech
token_type: sentencepiece
token_num: bpe5k

# model-related
label_smoothing: 0.2

d_model: 512
num_heads: 8
fdfwd_dim: 2048
dropout: 0.1

# optimizer-related
warmup_steps: 16000
### ------------------------ ###



model:
  model_type: ar_asr.ARASR
  model_conf:
    customize_conf:
      token_type: !ref <token_type>
      token_vocab: !ref <data_root>/<token_type>/<train_set>/<token_num>/<txt_format>/vocab

  module_conf:
    frontend:
      type: mel_fbank
      conf:
        sr: !ref <sample_rate>
        preemphasis: 0.97
        hop_length: 0.010
        win_length: 0.025
        n_mels: 80

    normalize: True

    specaug:
      freq_mask_width: 15
      freq_mask_num: 4
      time_mask_width: 20
      time_mask_num: 4

    enc_prenet:
      type: conv2d
      conf:
        conv_dims:
          - 64
          - 64
        conv_kernel: 3
        conv_stride: 2
        conv_batchnorm: true
        lnr_dims: !ref <d_model>

    encoder:
      type: transformer
      conf:
        posenc_dropout: !ref <dropout>
        posenc_scale: false
        emb_layernorm: true
        emb_scale: false
        d_model: !ref <d_model>
        num_heads: !ref <num_heads>
        num_layers: 12
        att_dropout: !ref <dropout>
        fdfwd_dim: !ref <fdfwd_dim>
        fdfwd_dropout: !ref <dropout>
        res_dropout: !ref <dropout>
        layernorm_first: true

    dec_emb:
      type: embed
      conf:
        embedding_dim: !ref <d_model>

    decoder:
      type: transformer
      conf:
        posenc_dropout: !ref <dropout>
        posenc_scale: false
        emb_layernorm: true
        emb_scale: false
        d_model: !ref <d_model>
        num_heads: !ref <num_heads>
        num_layers: 6
        att_dropout: !ref <dropout>
        fdfwd_dim: !ref <fdfwd_dim>
        fdfwd_dropout: !ref <dropout>
        res_dropout: !ref <dropout>
        layernorm_first: true

  criterion_conf:
    label_smoothing: !ref <label_smoothing>


optim_sches:
  type: noam.Noamlr
  conf:
    optim_type: Adam
    optim_conf:
      betas:
        - 0.9
        - 0.98
      eps: 1.0e-9
    warmup_steps: !ref <warmup_steps>