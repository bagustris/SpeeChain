##############################################################################
# Model: Transformer-ASR
# Tokens: BPE 5k
# losses: 0.2 Label Smooth
# Training: real train-clean-100 of LibriSpeech & synthetic train-clean-360 of 16khz-downsampled LibriTTS
# Validation: dev-clean of LibriSpeech
# Testing: test-clean & test-other of LibriSpeech
# Authors: Heli Qi
# Required GPUs: 3 Ã— NVIDIA RTX A5000(24GB)
# Required Time: up to 5 hours
# ############################################################################


###################################
# Experiment Parameters and setup #
###################################
# if your dumped dataset is outside the toolkit folder, please change real_dataset_path & syn_dataset_path.
# There should be a folder named 'libritts' in dataset_path
real_dataset_path: datasets/
syn_dataset_path: recipes/offline_tts2asr/tts_syn_speech/

# data-related
real_train_set: train-clean-100
syn_dataset: libritts
syn_train_set: train-clean-360
syn_wav_folder: seed=0_spk-emb=libritts-train-clean-100-ecapa_model=recipes%tts%libritts%train-clean-100%exp%16khz_ecapa_g2p_transformer-v3_accum1_20gb
valid_set: dev-clean

librispeech_data_root: !ref <real_dataset_path>/librispeech/data
libritts_data_root: !ref <real_dataset_path>/libritts/data
syn_data_root: !ref <syn_dataset_path>/<syn_dataset>/<syn_train_set>/default_inference/10_train_loss_average/<syn_wav_folder>

# waveform-related
librispeech_wav_format: wav
libritts_wav_format: wav16000
syn_wav_vocoder: gl
sample_rate: 16000

# tokenizer-related
txt_format: librispeech
token_type: sentencepiece
token_num: bpe1k

# batch-related values
real_batch_len: 9e6
syn_batch_len: 9e6

# model-related
norm_type: global
label_smoothing: 0.2
d_model: 512
num_heads: 8
fdfwd_dim: 2048
dropout: 0.1

# optimizer-related
warmup_steps: 16000

# running-related
seed: 0
num_workers: 1
pin_memory: True
non_blocking: True

# gradient-related
accum_grad: 1
ft_factor: 1.0
grad_clip: 5.0

# multi-GPU-related
ngpu: 3 # please change ngpu based on the situation of your machine
gpus: null # null means the GPUs with the largest free memory will be used

# training-related
train: False
best_model_selection: !tuple (valid, accuracy, max, 10)
early_stopping_patience: 10

# snapshot-related
visual_snapshot_number: 3
visual_snapshot_interval: 5

# testing-related
test: True
test_model: !ref <early_stopping_patience>_valid_accuracy_average



#################################
# Model Inference Configuration #
#################################
# Before evaluating the trained model on test-clean and test-other, we need to find the best softmax temperature on dev-clean.
# Here the searching range is all the odd number from 1.1 to 1.9.
infer_cfg:
  shared_args:
    beam_size: 16
  exclu_args:
    - temperature: 1.1
    - temperature: 1.3
    - temperature: 1.5
    - temperature: 1.7
    - temperature: 1.9



##############################
# Data Loading Configuration #
##############################
data_cfg:
  train:
    real:
      type: block.BlockIterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            feat:
              - !ref <librispeech_data_root>/<librispeech_wav_format>/<real_train_set>/idx2wav
              - !ref <libritts_data_root>/<libritts_wav_format>/<real_train_set>/idx2wav
            text:
              - !ref <librispeech_data_root>/<librispeech_wav_format>/<real_train_set>/idx2<txt_format>_text
              - !ref <libritts_data_root>/<libritts_wav_format>/<real_train_set>/idx2<txt_format>_text

        data_len:
          - !ref <librispeech_data_root>/<librispeech_wav_format>/<real_train_set>/idx2wav_len
          - !ref <libritts_data_root>/<libritts_wav_format>/<real_train_set>/idx2wav_len
        shuffle: True
        is_descending: True
        batch_len: !ref <real_batch_len>

    syn:
      type: block.BlockIterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            feat: !ref <syn_data_root>/idx2<syn_wav_vocoder>_wav
            text: !ref <real_dataset_path>/<syn_dataset>/data/<libritts_wav_format>/<syn_train_set>/idx2<txt_format>_text
          data_selection:
            - min
            - !str 8
            - !ref <syn_data_root>/idx2feat_token_len_ratio

        data_len: !ref <syn_data_root>/idx2<syn_wav_vocoder>_wav_len
        shuffle: True
        is_descending: True
        batch_len: !ref <syn_batch_len>

  valid:
    type: abs.Iterator
    conf:
      dataset_type: speech_text.SpeechTextDataset
      dataset_conf:
        main_data:
          feat: !ref <librispeech_data_root>/<librispeech_wav_format>/<valid_set>/idx2wav
          text: !ref <librispeech_data_root>/<librispeech_wav_format>/<valid_set>/idx2<txt_format>_text

      shuffle: False
      data_len: !ref <librispeech_data_root>/<librispeech_wav_format>/<valid_set>/idx2wav_len

  test:
    test-clean:
      type: abs.Iterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            feat: !ref <librispeech_data_root>/<librispeech_wav_format>/test-clean/idx2wav
            text: !ref <librispeech_data_root>/<librispeech_wav_format>/test-clean/idx2<txt_format>_text

        data_len: !ref <librispeech_data_root>/<librispeech_wav_format>/test-clean/idx2wav_len
        shuffle: False
        group_info:
          speaker: !ref <librispeech_data_root>/<librispeech_wav_format>/test-clean/idx2spk
          gender: !ref <librispeech_data_root>/<librispeech_wav_format>/test-clean/idx2gen

    test-other:
      type: abs.Iterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            feat: !ref <librispeech_data_root>/<librispeech_wav_format>/test-other/idx2wav
            text: !ref <librispeech_data_root>/<librispeech_wav_format>/test-other/idx2<txt_format>_text

        data_len: !ref <librispeech_data_root>/<librispeech_wav_format>/test-other/idx2wav_len
        shuffle: False
        group_info:
          speaker: !ref <librispeech_data_root>/<librispeech_wav_format>/test-other/idx2spk
          gender: !ref <librispeech_data_root>/<librispeech_wav_format>/test-other/idx2gen



####################################
# Model Construction Configuration #
####################################
train_cfg:
  model:
    model_type: ar_asr.MultiDomainARASR
    model_conf:
      customize_conf:
        token_type: !ref <token_type>
        token_vocab: !ref <librispeech_data_root>/<token_type>/<real_train_set>/<token_num>/<txt_format>/vocab

    module_conf:
      frontend:
        type: mel_fbank
        conf:
          sr: !ref <sample_rate>
          preemphasis: 0.97
          hop_length: 0.010
          win_length: 0.025
          n_mels: 80

      normalize:
        norm_type: !ref <norm_type>

      specaug:
        freq_mask_width: 15
        freq_mask_num: 4
        time_mask_width: 20
        time_mask_num: 4

      enc_prenet:
        type: conv2d
        conf:
          conv_dims:
            - 64
            - 64
          conv_kernel: 3
          conv_stride: 2
          conv_batchnorm: true
          lnr_dims: !ref <d_model>

      encoder:
        type: transformer
        conf:
          posenc_dropout: !ref <dropout>
          posenc_scale: false
          emb_layernorm: true
          emb_scale: false
          d_model: !ref <d_model>
          num_heads: !ref <num_heads>
          num_layers: 12
          att_dropout: !ref <dropout>
          fdfwd_dim: !ref <fdfwd_dim>
          fdfwd_dropout: !ref <dropout>
          res_dropout: !ref <dropout>
          layernorm_first: true

      dec_emb:
        type: embed
        conf:
          embedding_dim: !ref <d_model>

      decoder:
        type: transformer
        conf:
          posenc_dropout: !ref <dropout>
          posenc_scale: false
          emb_layernorm: true
          emb_scale: false
          d_model: !ref <d_model>
          num_heads: !ref <num_heads>
          num_layers: 6
          att_dropout: !ref <dropout>
          fdfwd_dim: !ref <fdfwd_dim>
          fdfwd_dropout: !ref <dropout>
          res_dropout: !ref <dropout>
          layernorm_first: true

    criterion_conf:
      ce_loss:
        label_smoothing: !ref <label_smoothing>

  optim_sches:
    type: noam.Noamlr
    conf:
      optim_type: Adam
      optim_conf:
        betas:
          - 0.9
          - 0.98
        eps: 1.0e-9
      warmup_steps: !ref <warmup_steps>
