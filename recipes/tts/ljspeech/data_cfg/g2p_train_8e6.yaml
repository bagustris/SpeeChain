### Author: Heli Qi
### Affiliation: NAIST
### Date: 2022.09
# This is a template of training configuration on the train_clean_460 dataset of the LibriSpeech corpus
# This configuration fits 4 GPUs with 10GB memory where each GPU processes one utterance at each validation and testing step
# The testing set is the same with the validation set, so that we can tune the decoding hyperparameters before the final evaluation


### --- Reference Zone --- ###
# data-related
dataset: librispeech
train_set: train
valid_set: valid

wav_format: wav
txt_format: normal
token_num: full_tokens
data_root: !ref datasets/<dataset>/data

# batch-related
batch_len: 8e6
### ---------------------- ###


### --- Training Iterator Configuration --- ###
train:
    type: block.BlockIterator
    conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
            main_data:
                feat: !ref <data_root>/<wav_format>/<train_set>/idx2wav
                text: !ref <data_root>/g2p/<train_set>/<token_num>/<txt_format>/idx2text

        data_len: !ref <data_root>/<wav_format>/<train_set>/idx2wav_len
        shuffle: True
        is_descending: True
        batch_len: !ref <batch_len>


### --- Validation Iterator Configuration --- ###
valid:
    type: abs.Iterator
    conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
            main_data:
                feat: !ref <data_root>/<wav_format>/<valid_set>/idx2wav
                text: !ref <data_root>/g2p/<valid_set>/<token_num>/<txt_format>/idx2text

        shuffle: False
        data_len: !ref <data_root>/<wav_format>/<valid_set>/idx2wav_len
